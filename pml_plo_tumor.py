# -*- coding: utf-8 -*-
"""PML_PLO

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_ymvTx5NASJX3YMwmffMxEhrTIQOESkh
"""

#install and import packages
#!pip install --upgrade scikit-learn
#!pip install pandas
import numpy as np
#import matplotlib.pyplot as plt

import sklearn
from sklearn.feature_selection import VarianceThreshold
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import Normalizer, StandardScaler, MinMaxScaler, PowerTransformer, MaxAbsScaler, LabelEncoder
from sklearn.model_selection import GridSearchCV

from sklearn import linear_model, ensemble
from sklearn.model_selection import cross_val_score
from sklearn import model_selection
import numpy

#suppress warnings about class imbalances
import warnings
warnings.filterwarnings("ignore")

from sklearn.preprocessing import OneHotEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import RidgeClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_validate
from sklearn.svm import SVC


from scipy.io import arff
import pandas as pd

data = arff.loadarff('primary-tumor.arff')
df = pd.DataFrame(data[0])
print(df.head())

print(df.dtypes)
print(df.columns)
print(df.info(verbose=True))
#read and display data
#data = pd.read_csv("winequality-red.csv", sep = ";")
#split into features/target
#for col in ['age', 'sex', 'histologic-type', 'degree-of-diffe', 'bone',
#       'bone-marrow', 'lung', 'pleura', 'peritoneum', 'liver', 'brain', 'skin',
#       'neck', 'supraclavicular', 'axillar', 'mediastinum', 'abdominal']:
#    df = df[df[col] != b'?']
#print(df.info(verbose=True))
x = df.drop(columns = ['binaryClass'])
from sklearn.preprocessing import MultiLabelBinarizer, LabelBinarizer, OneHotEncoder, OrdinalEncoder, label_binarize
#y = pd.DataFrame(MultiLabelBinarizer().fit_transform(df['binaryClass']))
#y = pd.DataFrame(label_binarize(df['binaryClass'], classes=["b'P'", "b'N'"]))
di = {b'P' : 1, b'N' : 0}
df['binaryClass'] = df['binaryClass'].map(di) 
y = df['binaryClass']
print(y)
#expand search space
#compare performance of different pipelines
#!pip install scikit-optimize
import skopt
from skopt.space import Real, Categorical, Integer
from skopt import BayesSearchCV

#define hyperparameter grids for each type of classifier
#https://scikit-optimize.github.io/stable/modules/generated/skopt.space.space.Real.html
#https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html

svc_param_grid = BayesSearchCV(SVC(),
      {
     'C': Real(1e-6, 1e+6, prior='log-uniform'),
         'gamma': Real(1e-6, 1e+1, prior='log-uniform'),
         'degree': Integer(1,8),
         'kernel': Categorical(['linear', 'poly', 'rbf']),
          },      n_iter=3,
          n_points = 3,
          n_jobs = 3, 
        random_state=0,
        scoring = "balanced_accuracy"
)
kn_param_grid = BayesSearchCV(KNeighborsClassifier(),
 {
        'n_neighbors' : Integer(1, 100, prior = 'log-uniform'),
        'algorithm' :  Categorical(['ball_tree', 'kd_tree', 'brute']),
        'leaf_size' : Integer(1, 50, prior='log-uniform'),
                },
        n_iter=3,
        n_points = 3, 
        n_jobs  = 3, 
        random_state=0,
        scoring = "balanced_accuracy"
                             )
ridge_param_grid = BayesSearchCV(RidgeClassifier(),
{
        'tol' : Real(0.01, 0.1, prior = 'log-uniform'),
        'solver' : Categorical(["sparse_cg", 'lsqr']),
        'alpha' : Real(0.1, 1.0, prior = 'log-uniform'),
                },
        n_iter=3,
        n_points = 3,
        n_jobs  = 3,
        random_state=0,
        scoring = "balanced_accuracy"
                                )

dt_param_grid = BayesSearchCV(DecisionTreeClassifier(),
{
        'max_depth' : Integer(1, 10, prior = 'log-uniform'),
        'max_features' : Categorical([None, "auto", "sqrt", "log2"]),
        'min_samples_split':Real(0.1, 1.0, prior = 'log-uniform'),
                },
        n_iter=3,
        n_points = 3,
        n_jobs  = 3,
        random_state=0,
        scoring = "balanced_accuracy"
                             )

bagging_param_grid = BayesSearchCV(ensemble.BaggingClassifier(),
                                  {
    "n_estimators" : Integer(50, 500, prior = 'log-uniform'),
    "max_features" : Real(0.1, 5, prior = 'log-uniform'),
},
                                    n_iter=3,n_points = 3,
        random_state=0,
        n_jobs  = 3,
        scoring = "balanced_accuracy")

random_forest_param_grid = BayesSearchCV(ensemble.RandomForestClassifier(),
 {"n_estimators" : Integer(100, 100000, prior = 'log-uniform'),
    "criterion" : Categorical(["gini", "entropy", "log_loss"]),
    "max_depth" : Integer(1, 10, prior = "log-uniform"),
},

 n_iter=3,
 n_points = 3,
 n_jobs  = 3,
        random_state=0,
        scoring = "balanced_accuracy"
                             )
                             



vt_param_grid = BayesSearchCV(VarianceThreshold(), 
{"threshold" : Real(0.0, 10)}, 
    n_iter = 3, random_state = 0
)
#construct a pipeline with a scaler, encoder, feature selector, and estimator/classifier
pipe = Pipeline([
    #('scaler', StandardScaler()),
    ('encoder', OneHotEncoder()),
    ('selector', VarianceThreshold()),
    ('estimator', KNeighborsClassifier())
])

#define a grid search over different estimators in the pipeline
grid = GridSearchCV(
    estimator=pipe,
    param_grid={
        #"scaler": [StandardScaler(), MinMaxScaler(), Normalizer(), MaxAbsScaler(), "passthrough"],
        'encoder': [OneHotEncoder(), OrdinalEncoder()],
        'selector'  : [vt_param_grid, "passthrough"],
        'estimator': [ridge_param_grid, kn_param_grid, dt_param_grid, bagging_param_grid, random_forest_param_grid],
    },
    n_jobs = -1,
    scoring = 'balanced_accuracy',
    cv = 3,
    return_train_score = True
)

cv_results = cross_validate(
        grid, x, y, cv=5, return_estimator=True, scoring = "balanced_accuracy", n_jobs = -1
    )
cv_results_df = pd.DataFrame(cv_results)
cv_test_scores = cv_results_df["test_score"]
#display results
print(
        "Generalization score with hyperparameters tuning:\n"
        f"{cv_test_scores.mean():.3f} Â± {cv_test_scores.std():.3f}"
    )
#display best hyperparameter configuration
max_idx  = cv_results_df['test_score'].idxmax()
print(cv_results["estimator"][max_idx].best_estimator_)
print(cv_results["encoder"][max_idx].best_estimator_.best_estimator_)
print(cv_results["selector"][max_idx].best_estimator_.best_estimator_)
print(cv_results["estimator"][max_idx].best_estimator_.best_estimator_)
print(cv_results["estimator"][max_idx].best_estimator_.best_estimator_.best_estimator)