# -*- coding: utf-8 -*-
"""PML_PLO

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_ymvTx5NASJX3YMwmffMxEhrTIQOESkh
"""

#install and import packages
#!pip install --upgrade scikit-learn
#!pip install pandas
import pandas as pd
import numpy as np
#import matplotlib.pyplot as plt
pd.set_option('display.width', None)
pd.set_option('max_colwidth', None)
import sklearn
from sklearn.feature_selection import VarianceThreshold
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import Normalizer, StandardScaler, MinMaxScaler, PowerTransformer, MaxAbsScaler, LabelEncoder
from sklearn.model_selection import GridSearchCV

from sklearn import linear_model, ensemble
from sklearn.model_selection import cross_val_score
from sklearn import model_selection
import numpy

#suppress warnings about class imbalances
import warnings
warnings.filterwarnings("ignore")

from sklearn.preprocessing import OneHotEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import RidgeClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_validate
from sklearn.svm import SVC

#read and display data
data = pd.read_csv("winequality-red.csv", sep = ";")
#split into features/target
x = data.drop(columns = ['quality'])
y = data['quality']

#expand search space
#compare performance of different pipelines

#!pip install scikit-optimize
import skopt
from skopt.space import Real, Categorical, Integer
from skopt import BayesSearchCV

#define hyperparameter grids for each type of classifier
#https://scikit-optimize.github.io/stable/modules/generated/skopt.space.space.Real.html
#https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html

svc_param_grid = BayesSearchCV(SVC(),
      {
     'C': Real(1e-6, 1e+6, prior='log-uniform'),
         'gamma': Real(1e-6, 1e+1, prior='log-uniform'),
         'degree': Integer(1,8),
         'kernel': Categorical(['linear', 'poly', 'rbf']),
          },      n_iter=3,
          n_jobs = 3, 
          n_points = 3, 
        random_state=0,
        scoring = "balanced_accuracy"
)
kn_param_grid = BayesSearchCV(KNeighborsClassifier(),
 {
        'n_neighbors' : Integer(1, 100, prior = 'log-uniform'),
        'algorithm' :  Categorical(['ball_tree', 'kd_tree', 'brute']),
        'leaf_size' : Integer(1, 50, prior='log-uniform'),
                },
        n_iter=3,
        n_jobs  = 3, 
        random_state=0,
        scoring = "balanced_accuracy"
                             )
ridge_param_grid = BayesSearchCV(RidgeClassifier(),
{
        'tol' : Real(0.01, 0.1, prior = 'log-uniform'),
        'solver' : Categorical(["svd", "cholesky","sparse_cg", 'saga', 'lsqr']),
        'alpha' : Real(0.1, 1.0, prior = 'log-uniform'),
                },
       n_iter=3,
          n_jobs = 3, 
          n_points = 3, 
        random_state=0,
        scoring = "balanced_accuracy"
                                )

dt_param_grid = BayesSearchCV(DecisionTreeClassifier(),
{
        'max_depth' : Integer(1, 10, prior = 'log-uniform'),
        'max_features' : Categorical([None, "auto", "sqrt", "log2"]),
        'min_samples_split':Real(0.1, 1.0, prior = 'log-uniform'),
                },
        n_iter=3,
          n_jobs = 3, 
          n_points = 3, 
        random_state=0,
        scoring = "balanced_accuracy"
                             )

bagging_param_grid = BayesSearchCV(ensemble.BaggingClassifier(),
                                  {
    "n_estimators" : Integer(50, 500, prior = 'log-uniform'),
    "max_features" : Real(0.1, 5, prior = 'log-uniform'),
},
                                    n_iter=3,
          n_jobs = 3, 
          n_points = 3, random_state = 0,
        scoring = "balanced_accuracy")

random_forest_param_grid = BayesSearchCV(ensemble.RandomForestClassifier(),
 {"n_estimators" : Integer(100, 100000, prior = 'log-uniform'),
    "criterion" : Categorical(["gini", "entropy", "log_loss"]),
    "max_depth" : Integer(1, 10, prior = "log-uniform"),
},

n_iter=3,
          n_jobs = 3, 
          n_points = 3, 
        random_state=0,
        scoring = "balanced_accuracy"
                             )
                             
standard_scaler_param_grid = BayesSearchCV(StandardScaler(), 
    {"with_std": Categorical([True, False])}, 
    n_iter = 3, random_state = 0)
    
norm_param_grid = BayesSearchCV(Normalizer(), 
{"norm": Categorical(['l1', 'l2', 'max'])}, 
n_iter = 3, random_state = 0)

vt_param_grid = BayesSearchCV(VarianceThreshold(), 
{"threshold" : Real(0.0, 10)}, 
    n_iter = 3, random_state = 0
)
#construct a pipeline with a scaler, encoder, feature selector, and estimator/classifier
pipe = Pipeline([
    ('scaler', StandardScaler()),
    #('onehot', OneHotEncoder()),
    ('selector', VarianceThreshold()),
    ('estimator', KNeighborsClassifier())
])

#train/test split
#x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(x, y, test_size=0.2, random_state=42)

#define a grid search over different estimators in the pipeline
grid = GridSearchCV(
    estimator=pipe,
    param_grid={
        "scaler": [standard_scaler_param_grid, MinMaxScaler(), norm_param_grid, MaxAbsScaler(), "passthrough"],
        #"onehot": [OneHotEncoder(), "passthrough"],
        "selector"  : [vt_param_grid, "passthrough"],
        'estimator': [ridge_param_grid, kn_param_grid, dt_param_grid, bagging_param_grid, random_forest_param_grid],
    },
    n_jobs = -1,
    scoring = 'balanced_accuracy',
    cv = 3,
    return_train_score = True
)

cv_results = cross_validate(
        grid, x, y, cv=5, return_estimator=True, scoring = "balanced_accuracy", n_jobs = -1
    )
cv_results_df = pd.DataFrame(cv_results)
cv_test_scores = cv_results_df["test_score"]

#display results
print(
        "Generalization score with hyperparameters tuning:\n"
        f"{cv_test_scores.mean():.3f} Â± {cv_test_scores.std():.3f}"
    )
print(cv_results_df.loc[cv_results_df['test_score'].idxmax()])
max_idx  = cv_results_df['test_score'].idxmax()
print(cv_results["estimator"][max_idx].best_estimator_)
print(cv_results["scaler"][max_idx].best_estimator_.best_estimator_)
print(cv_results["selector"][max_idx].best_estimator_.best_estimator_)
print(cv_results["estimator"][max_idx].best_estimator_.best_estimator_)